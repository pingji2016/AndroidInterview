å¥½çš„ï¼Œæˆ‘æ¥è¯¦ç»†å¯¹æ¯”è¿™ä¸‰ç§é‡è¦çš„æ ‘æ¨¡å‹ç®—æ³•ï¼š

## ğŸŒ³ ç®—æ³•ç‰¹æ€§å¯¹æ¯”

| ç‰¹æ€§ | å†³ç­–æ ‘ | éšæœºæ£®æ— | æ¢¯åº¦æå‡æ ‘ (GBM) |
|------|--------|----------|------------------|
| **åŸºæœ¬åŸç†** | å•ä¸€æ ‘ç»“æ„ | å¤šæ£µæ ‘çš„baggingé›†æˆ | å¤šæ£µæ ‘çš„boostingé›†æˆ |
| **è®­ç»ƒæ–¹å¼** | ä¸€æ¬¡æ€§æ„å»º | å¹¶è¡Œè®­ç»ƒå¤šæ£µæ ‘ | é¡ºåºè®­ç»ƒï¼Œæ¯æ£µæ ‘ä¿®æ­£å‰ä¸€æ£µçš„é”™è¯¯ |
| **é¢„æµ‹é€Ÿåº¦** | âš¡âš¡âš¡ æœ€å¿« | âš¡âš¡ ä¸­ç­‰ | âš¡ è¾ƒæ…¢ |
| **è®­ç»ƒé€Ÿåº¦** | âš¡âš¡âš¡ æœ€å¿« | âš¡âš¡ ä¸­ç­‰ | âš¡ æœ€æ…¢ |
| **è¿‡æ‹Ÿåˆé£é™©** | ğŸ”´ å¾ˆé«˜ | ğŸŸ¡ ä¸­ç­‰ | ğŸŸ¢ è¾ƒä½ï¼ˆæœ‰æ—©åœï¼‰ |
| **é¢„æµ‹ç²¾åº¦** | ğŸ”´ é€šå¸¸è¾ƒä½ | ğŸŸ¡ è‰¯å¥½ | ğŸŸ¢ é€šå¸¸æœ€é«˜ |
| **å¯è§£é‡Šæ€§** | ğŸŸ¢ æœ€å¥½ | ğŸŸ¡ ä¸­ç­‰ | ğŸ”´ è¾ƒå·® |

## ğŸ¯ æ ¸å¿ƒåŒºåˆ«è¯¦è§£

### 1. å†³ç­–æ ‘ (Decision Tree)
```python
# ç®€å•æ˜“æ‡‚ï¼Œä½†å®¹æ˜“è¿‡æ‹Ÿåˆ
model = tfdf.keras.CartModel(
    max_depth=5,  # éœ€è¦æ‰‹åŠ¨é™åˆ¶æ·±åº¦é˜²è¿‡æ‹Ÿåˆ
    min_examples=10
)
```
**é€‚ç”¨åœºæ™¯**ï¼šå¿«é€ŸåŸå‹ã€å¯è§£é‡Šæ€§è¦æ±‚é«˜ã€ç®€å•é—®é¢˜

### 2. éšæœºæ£®æ— (Random Forest)
```python
# é€šè¿‡baggingå‡å°‘æ–¹å·®
model = tfdf.keras.RandomForestModel(
    num_trees=100,           # å¤šæ£µæ ‘æŠ•ç¥¨
    subsample=0.8,           # è¡Œé‡‡æ ·
    num_candidate_attributes_ratio=0.5  # åˆ—é‡‡æ ·
)
```
**é€‚ç”¨åœºæ™¯**ï¼šé€šç”¨æ€§å¼ºã€éœ€è¦ç¨³å®šæ€§èƒ½ã€ä¸­ç­‰å¤æ‚åº¦é—®é¢˜

### 3. æ¢¯åº¦æå‡æ ‘ (Gradient Boosted Trees)
```python
# é€šè¿‡boostingé™ä½åå·®
model = tfdf.keras.GradientBoostedTreesModel(
    num_trees=1000,         # æ›´å¤šä½†æ›´æµ…çš„æ ‘
    shrinkage=0.1,          # å­¦ä¹ ç‡æ§åˆ¶
    early_stopping="LOSS_INCREASE"  # é˜²è¿‡æ‹Ÿåˆ
)
```
**é€‚ç”¨åœºæ™¯**ï¼šé«˜ç²¾åº¦è¦æ±‚ã€å¤æ‚æ¨¡å¼è¯†åˆ«ã€ç«èµ›åœºæ™¯

## ğŸ“Š æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹

```python
import tensorflow_decision_forests as tfdf
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# å‡†å¤‡æ•°æ®
X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.2)

# 1. å†³ç­–æ ‘
dt_model = tfdf.keras.CartModel(verbose=0)
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_val)
print(f"å†³ç­–æ ‘å‡†ç¡®ç‡: {accuracy_score(y_val, dt_pred):.4f}")

# 2. éšæœºæ£®æ—
rf_model = tfdf.keras.RandomForestModel(num_trees=100, verbose=0)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_val)
print(f"éšæœºæ£®æ—å‡†ç¡®ç‡: {accuracy_score(y_val, rf_pred):.4f}")

# 3. æ¢¯åº¦æå‡æ ‘
gbm_model = tfdf.keras.GradientBoostedTreesModel(num_trees=1000, verbose=0)
gbm_model.fit(X_train, y_train)
gbm_pred = gbm_model.predict(X_val)
print(f"GBMå‡†ç¡®ç‡: {accuracy_score(y_val, gbm_pred):.4f}")
```

## ğŸª å¦‚ä½•é€‰æ‹©ï¼Ÿ

### é€‰æ‹©å†³ç­–æ ‘å½“ï¼š
- âœ… éœ€è¦æ¨¡å‹å®Œå…¨å¯è§£é‡Š
- âœ… å¿«é€ŸéªŒè¯æƒ³æ³•
- âœ… æ•°æ®é‡å¾ˆå°
- âœ… ä½œä¸ºåŸºçº¿æ¨¡å‹

### é€‰æ‹©éšæœºæ£®æ—å½“ï¼š
- âœ… éœ€è¦ç¨³å®šå¯é çš„æ€§èƒ½
- âœ… ä¸æƒ³èŠ±æ—¶é—´è°ƒå‚
- âœ… æ•°æ®æœ‰è¾ƒå¤šå™ªå£°
- âœ… å¹¶è¡Œè®­ç»ƒé€Ÿåº¦é‡è¦

### é€‰æ‹©æ¢¯åº¦æå‡æ ‘å½“ï¼š
- âœ… è¿½æ±‚æœ€é«˜é¢„æµ‹ç²¾åº¦
- âœ… æœ‰è¶³å¤Ÿæ—¶é—´è°ƒå‚å’Œè®­ç»ƒ
- âœ… æ•°æ®æ¨¡å¼å¤æ‚
- âœ… å‚åŠ æœºå™¨å­¦ä¹ ç«èµ›

## ğŸ”§ è°ƒå‚é‡ç‚¹

| æ¨¡å‹ | å…³é”®å‚æ•° | è°ƒå‚å»ºè®® |
|------|----------|----------|
| **å†³ç­–æ ‘** | `max_depth`, `min_samples_split` | é‡ç‚¹é˜²æ­¢è¿‡æ‹Ÿåˆ |
| **éšæœºæ£®æ—** | `n_estimators`, `max_features` | å¢åŠ æ ‘çš„æ•°é‡ï¼Œæ§åˆ¶ç‰¹å¾é‡‡æ · |
| **æ¢¯åº¦æå‡æ ‘** | `n_estimators`, `learning_rate`, `max_depth` | å¹³è¡¡æ ‘çš„æ•°é‡å’Œå­¦ä¹ ç‡ |

## ğŸ’¡ å®è·µå»ºè®®

1. **ä»éšæœºæ£®æ—å¼€å§‹** - é€šå¸¸æ˜¯æœ€ä½³èµ·ç‚¹
2. **æ•°æ®é‡å°ç”¨éšæœºæ£®æ—** - GBMå®¹æ˜“è¿‡æ‹Ÿåˆå°æ•°æ®
3. **è¿½æ±‚ç²¾åº¦ç”¨GBM** - ä½†éœ€è¦ä»”ç»†è°ƒå‚å’ŒéªŒè¯
4. **éœ€è¦è§£é‡Šæ€§ç”¨å•æ ‘** - ä½†æ€§èƒ½é€šå¸¸è¾ƒå·®

åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œæˆ‘é€šå¸¸å…ˆå°è¯•éšæœºæ£®æ—ä½œä¸ºåŸºçº¿ï¼Œå¦‚æœç²¾åº¦ä¸å¤Ÿå†å°è¯•æ¢¯åº¦æå‡æ ‘ã€‚



pyä»£ç è¿™ä¸¤ä¸ªæ¨¡å‹é…ç½®æœ‰æ˜¾è‘—çš„åŒºåˆ«ï¼Œä¸»è¦ä½“ç°åœ¨**è¶…å‚æ•°è®¾ç½®å’Œæ¨¡å‹å¤æ‚åº¦**ä¸Šï¼š

## ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆå¤æ‚é…ç½®ï¼‰

```python
model = tfdf.keras.GradientBoostedTreesModel(
    verbose=0,
    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],
    exclude_non_specified_features=True,
    min_examples=1,                    # âš¡ æ›´å®¹æ˜“è¿‡æ‹Ÿåˆ
    categorical_algorithm="RANDOM",    # âš¡ åˆ†ç±»ç‰¹å¾å¤„ç†æ–¹å¼
    shrinkage=0.05,                    # âš¡ è¾ƒå°çš„å­¦ä¹ ç‡
    split_axis="SPARSE_OBLIQUE",       # âš¡ å¤æ‚çš„åˆ†å‰²æ–¹å¼
    sparse_oblique_normalization="MIN_MAX",
    sparse_oblique_num_projections_exponent=2.0,
    num_trees=2000,                    # âš¡ æ›´å¤šçš„æ ‘
    random_seed=1234,
)
```

## ç¬¬äºŒä¸ªæ¨¡å‹ï¼ˆç®€å•é»˜è®¤é…ç½®ï¼‰

```python
model = tfdf.keras.GradientBoostedTreesModel(
    verbose=0,
    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],
    exclude_non_specified_features=True,
    random_seed=1234,                  # ä»…è®¾ç½®éšæœºç§å­
    # å…¶ä»–å‚æ•°ä½¿ç”¨é»˜è®¤å€¼
)
```

## ä¸»è¦åŒºåˆ«å¯¹æ¯”ï¼š

| å‚æ•° | ç¬¬ä¸€ä¸ªæ¨¡å‹ | ç¬¬äºŒä¸ªæ¨¡å‹ | å½±å“ |
|------|------------|------------|------|
| **min_examples** | 1 | é»˜è®¤å€¼(å¯èƒ½ä¸º5-10) | âš ï¸ æ›´å®¹æ˜“è¿‡æ‹Ÿåˆ |
| **shrinkage** | 0.05 | é»˜è®¤å€¼(å¯èƒ½ä¸º0.1) | ğŸ“‰ å­¦ä¹ æ›´æ…¢ä½†æ›´ç²¾ç»† |
| **num_trees** | 2000 | é»˜è®¤å€¼(å¯èƒ½ä¸º300) | ğŸŒ³ æ¨¡å‹æ›´å¤æ‚ |
| **split_axis** | SPARSE_OBLIQUE | é»˜è®¤(AXIS_ALIGNED) | ğŸ¯ æ›´å¤æ‚çš„åˆ†å‰²ç­–ç•¥ |
| **categorical_algorithm** | RANDOM | é»˜è®¤ | ğŸ”€ ä¸åŒçš„åˆ†ç±»å¤„ç† |

## å®é™…å½±å“ï¼š

1. **æ¨¡å‹å¤æ‚åº¦**ï¼šç¬¬ä¸€ä¸ªæ¨¡å‹æ›´å¤æ‚ï¼Œæœ‰2000æ£µæ ‘
2. **è®­ç»ƒæ—¶é—´**ï¼šç¬¬ä¸€ä¸ªæ¨¡å‹è®­ç»ƒæ—¶é—´æ›´é•¿
3. **è¿‡æ‹Ÿåˆé£é™©**ï¼šç¬¬ä¸€ä¸ªæ¨¡å‹`min_examples=1`æ›´å®¹æ˜“è¿‡æ‹Ÿåˆ
4. **ç‰¹å¾äº¤äº’**ï¼š`SPARSE_OBLIQUE`åˆ†å‰²å¯ä»¥æ•æ‰æ›´å¤æ‚çš„ç‰¹å¾äº¤äº’
5. **æ”¶æ•›é€Ÿåº¦**ï¼š`shrinkage=0.05`ä½¿å­¦ä¹ æ›´æ…¢ä½†å¯èƒ½æ›´ç²¾ç¡®

## æ¨èé€‰æ‹©ï¼š

- **å¦‚æœæ•°æ®é‡å°**ï¼šä½¿ç”¨ç¬¬äºŒä¸ªç®€å•é…ç½®ï¼Œé¿å…è¿‡æ‹Ÿåˆ
- **å¦‚æœæ•°æ®é‡å¤§**ï¼šç¬¬ä¸€ä¸ªå¤æ‚é…ç½®å¯èƒ½è·å¾—æ›´å¥½æ€§èƒ½
- **å¦‚æœéœ€è¦å¯è§£é‡Šæ€§**ï¼šç¬¬äºŒä¸ªé…ç½®æ›´ç®€å•æ˜“æ‡‚
- **å¦‚æœè¿½æ±‚æœ€ä½³ç²¾åº¦**ï¼šç¬¬ä¸€ä¸ªé…ç½®ç»è¿‡ç²¾å¿ƒè°ƒå‚

æ‚¨å¯ä»¥é€šè¿‡äº¤å‰éªŒè¯æ¥æ¯”è¾ƒä¸¤ä¸ªé…ç½®çš„å®é™…è¡¨ç°ï¼Œé€‰æ‹©æœ€é€‚åˆæ‚¨æ•°æ®çš„æ¨¡å‹ã€‚