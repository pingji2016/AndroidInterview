# 端侧大模型工程化实践：小爱同学的技术突破

> **InfoQ专访：小米/小爱同学端侧AI负责人杨永杰深度解析**

随着大模型能力持续提升，如何将其有效部署到端侧设备，成为产业界面临的重要工程挑战。手机、车载、IoT等设备对模型体积、推理时延、功耗和更新机制都提出了极高要求，也让端侧推理成为融合系统优化、模型压缩和软硬件协同的复杂问题。

## 🎯 核心成果

近日，InfoQ对话小米/小爱同学端侧AI负责人杨永杰，深入了解其团队如何从架构、系统和算法三层着手，推进大模型在端侧的工程化落地：

### 📊 技术突破指标

- **推理性能**: 180 tokens/s 的实时推理速度
- **架构创新**: LoRA插件化 + 共享基座模型
- **业务支持**: 多业务复用，极致优化资源占用

### 🔮 未来展望

面向未来，杨永杰认为，端侧大模型的突破将依赖两方面：

1. **硬件能力提升**: 面向大模型优化的硬件能力
2. **模型架构演进**: Linear Attention架构等新技术

---

## 📅 技术分享预告

**6月27～28日**，在即将于北京举办的AICon全球人工智能开发与应用大会上，杨永杰将发表演讲《小爱同学在高性能端侧大模型推理的实践》，分享其团队自研的大模型推理框架在实际业务中的落地实践。

### 🎤 演讲内容

围绕以下方面展开，结合真实的系统优化痛点，解析端侧大模型商业化的关键路径：

- 架构设计
- 量化策略  
- 并行解码
- 跨芯片兼容
- 热更新策划

> **🔗 敬请期待**: [https://aicon.infoq.cn/2025/beijing/presentation/6444](https://aicon.infoq.cn/2025/beijing/presentation/6444)



---

## 💬 深度访谈

### Q1: 端侧大模型商业化部署的核心技术门槛

**InfoQ**: 端侧大模型已成为当前AI部署的重点方向，尤其在隐私、安全和离线能力方面具备显著优势。但从您的视角看，真正实现商业化部署仍面临哪些核心技术门槛？

**杨永杰**: 确实现在大模型很火，端侧也被很多人看作是未来的重要方向，但在商业化落地这方面推进得还是比较慢的，这主要是受到一些客观因素的影响。

#### 🔧 技术挑战分析

**1. 端侧设备资源限制**

- **算力限制**: 相比云端，端侧设备的算力和带宽都比较有限
- **量化要求**: 必须对模型进行较低比特数的量化才能在设备上运行
- **参数量限制**: 可商业化部署的模型难以超过4B参数量
- **效果损失**: 低比特量化会导致模型效果损失

> **现状**: 在这个量化精度下，大模型的整体效果相比云端仍有较大差距

**2. 模型迭代速度挑战**

- **云端优势**: 云端可以快速更新模型版本
- **端侧滞后**: 端侧设备在用户手上，模型更新相对滞后
- **更新机制受限**: 依赖用户主动下载或厂商推送更新包
- **控制灵活性**: 整体更新策略没有云端灵活

#### 🎯 发展现状与未来

**当前阶段**: 大模型的发展还没有到一个"相对稳定"的阶段

**技术积累**: 现在的端侧大模型更像是在做技术积累，是面向未来的准备

**未来趋势**: 当部署成本成为关键因素时，端侧就会成为更有吸引力的选项

> **我们的策略**: 提前技术布局，为之后的落地做好准备



### Q2: 自研推理框架的技术突破

**InfoQ**: 您所在团队自研了大模型推理框架，并实现了超过180 tokens/s的端侧推理速度，在端侧这一资源受限环境下达到该指标，能否分享一下背后的系统级与模型级优化策略？

**杨永杰**: 是的，我们团队自研了一个用于大模型推理的框架。之所以选择自研，主要是因为目前针对端侧的大模型推理框架非常少，开源的方案更是寥寥无几。

#### 🏗️ 自研框架的必要性

**现状分析**:

| 平台类型 | 框架支持 | 优化深度 | 社区生态 |
|---------|---------|---------|---------|
| **云端** | vLLM、SGLang等开源框架 | 深度优化，手段丰富 | 广泛社区贡献 |
| **端侧CPU/GPU** | 少量开源方案 | 基础支持 | 社区有限 |
| **端侧NPU** | 芯片厂商自研 | 实用性强，缺乏系统性打磨 | 厂商封闭 |

**核心问题**: 
- NPU接口不开放，推理框架只能由芯片厂商开发维护
- 端侧设备差异性大，各厂商解决方案分散
- 缺乏系统性打磨与性能极致优化

#### ⚡ 核心优化技术

**1. 动态输入与动态Context支持**

- **问题**: 端侧NPU通常只能使用静态图，输入尺寸固定
- **传统方案**: 将输入padding到固定长度（如128 token）
- **我们的方案**: 框架级能力支持动态输入，自动切分输入尺寸
- **效果**: 大幅提升资源利用率和吞吐率

**2. 投机推理（Speculative Decoding）优化**

- **云端效果**: Medusa或Eagle等方案通常2～3倍加速
- **我们的突破**: 端侧实现高达7～10倍的decoding加速
- **实际效果**: 从二十几tokens/s提升到200 tokens/s
- **意义**: 这一速度已经可以媲美部分云端场景

**3. 量化与指令级优化**

- **CPU操作**: 通过Neon指令集实现加速
- **优化范围**: 量化与反量化、sampling采样等

#### 🎯 落地成果

目前，我们的框架已在车载等端侧场景中实现了落地，支持包括文本和多模态在内的多个大模型能力。



### Q3: 小爱同学业务场景的架构约束

**InfoQ**: 小爱同学作为一个语音助手，承载了语音控制、多轮对话、智能家居等多种任务。端侧大模型在这些不同场景下对响应时延、并发能力有何特殊要求？这些业务需求对推理架构设计起到了怎样的约束作用？

**杨永杰**: 对于小爱来说，目前我们还没有非常强的并发需求，但这个在未来可能会出现。

#### 🔄 业务处理流程

**完整请求链路**:

```
感知 → 理解 → 满足
```

- **感知**: 设备感知用户输入
- **理解**: 进行语义理解  
- **满足**: 给出响应满足用户需求

**执行特点**: 这个流程本身是串行执行的，所以在同一条链路上，并发的需求并不强。

#### ⚡ 并发处理现状

**当前并发情况**:

| 并发类型 | 现状 | 原因 |
|---------|------|------|
| **单业务内部** | 并发需求不强 | 串行执行流程 |
| **跨业务并发** | 存在并发冲突 | 模型能力被其他业务共用 |
| **硬件支持** | NPU不支持并发推理 | 硬件设计以串行执行为主 |

#### 🏗️ 架构设计约束

**1. 硬件限制**

- **NPU特性**: 端侧设备的NPU本身不支持并发推理
- **硬件设计**: 以串行执行为主，不像云端天然支持多请求并发处理

**2. Multi-batch策略分析**

- **理论方案**: 尝试使用multi-batch方式提升并发处理效率
- **实际效果**: 在端侧算力受限环境下，multi-batch收益非常有限
- **原因分析**: 端侧单条请求已接近设备算力上限，增加batch数反而带来额外负担

> **实验结论**: 在预填阶段（prefill）已经接近满负载，再加一条并不会带来实质提升，几乎和一条条串行处理没区别

**3. 调度优化策略**

- **调度机制**: 通过调度和切换机制保障各条业务链路在预期时间内完成推理
- **资源管理**: 避免某条请求因为资源冲突而变得特别慢
- **性能平衡**: 在硬件限制下实现最优的资源分配



### Q4: 共享基座架构的设计理念

**InfoQ**: 面对多任务、多业务的实际场景，您团队采用了"共享基座架构"的方案。能否大概介绍一下，该架构方案是如何支持多个业务并发推理的，同时又不牺牲系统性能？

**杨永杰**: 我们之所以要做共享基座架构，主要是因为端侧的资源确实有限，不仅是算力有限，存储空间和内存也很受限制。

#### 💾 资源限制分析

**内存占用实例**:

- **设备配置**: 12GB内存的手机
- **模型需求**: 部署一个4B大模型需要接近3GB内存
- **实际可用**: 扣除其他业务占用，真正能留给大模型的空间可能连3GB都不到

> **核心问题**: 无法为每个业务分别部署一个独立模型

#### 🏗️ 共享基座架构设计

**设计思路**: "共享基座模型 + 插件化能力"

**架构组成**:

```
基础大模型 (共享)
├── A业务LoRA模块
├── B业务LoRA模块  
└── C业务LoRA模块
```

**实现方案**:

1. **统一基础模型**: 为所有业务选择同一个基础大模型
2. **差异化训练**: 针对不同业务单独训练对应的LoRA模块
3. **动态切换**: 运行时根据业务请求动态加载对应的LoRA

#### ⚡ 运行机制

**动态切换流程**:

| 步骤 | A业务请求 | B业务请求 |
|------|----------|----------|
| 1 | 加载基础模型 + A的LoRA | 卸载A的LoRA |
| 2 | 执行推理 | 加载B的LoRA |
| 3 | 返回结果 | 执行推理 |

**核心优势**:

- **参数共享**: 多个业务共用同一个基础模型
- **差异定制**: 通过LoRA实现业务特定能力
- **资源高效**: 在资源有限的设备上支持多个任务
- **扩展性强**: 内存利用率和扩展能力都有优势

> **技术挑战**: 实际落地过程并不像听起来那么简单，里面还有不少实现上的细节和挑战



### Q5: 跨芯片平台部署的通用化设计

**InfoQ**: 端侧设备硬件异构严重，适配难度大。你们的推理框架在跨芯片平台部署上做了哪些模块化、通用化的设计，以确保兼容性与性能的平衡？

**杨永杰**: 在这方面其实我们并没有采用特别的方案。因为从整体设计思路上看，这跟传统模型框架处理多后端的问题是类似的。

#### 🔄 设计思路分析

**大模型vs传统模型**:

| 方面 | 传统模型框架 | 大模型框架 |
|------|-------------|-----------|
| **模型规模** | 相对较小 | 规模显著增大 |
| **框架设计** | 多后端适配复杂 | 本质性变化不大 |
| **优化重点** | 硬件绑定较深 | 更多针对模型结构特性 |
| **通用性** | 适配难度大 | 更容易抽象通用接口 |

#### 🏗️ 通用化设计优势

**1. 抽象层设计**

- **接口统一**: 框架层更容易抽象出通用接口
- **平台迁移**: 模型能在不同硬件平台、不同后端之间迁移
- **通用性提升**: 相比传统模型框架，更容易实现跨平台部署能力

**2. 模块化策略**

- **后端解耦**: 进行模块化、后端解耦的设计
- **平台适配**: 适应多种端侧芯片平台的部署需求
- **统一接口**: 上层调用时底层适配逻辑已统一封装

> **核心观点**: 大模型虽然规模变大，但在框架设计层面上并没有本质性变化，反而更容易实现通用性和跨平台部署能力



### Q6: 性能优化技术的组合策略

**InfoQ**: 在性能优化方面，你们使用了如低比特量化、并行解码、带宽控制等技术。实际工程中，这些优化组合的优先级是如何确定的？是否有一些踩过的坑可以分享？

**杨永杰**: 我们现在在做优化时，基本上不同的技术是可以同时作用的，不会出现只能用A而不能用B的情况。

#### 🔧 技术组合策略

**优化技术兼容性**:

- **低比特量化** ✅ 可组合使用
- **并行解码** ✅ 可组合使用  
- **带宽控制** ✅ 可组合使用

**组合原则**: 这些技术我们都是尽可能组合使用的，比如策略A和策略B不仅能分别使用，也可以将它们融合在一起。

#### 📊 优先级确定标准

**判断标准**:

1. **技术价值**: 优化效果是否显著
2. **适用面**: 能否在多个业务场景中复用
3. **兼容性**: 是否容易与其他优化组合应用
4. **复杂度**: 实现起来不增加系统复杂度

**优先级策略**:

| 优先级 | 技术特征 | 选择标准 |
|--------|---------|---------|
| **高** | 技术价值大、适用面广、无冲突 | 优先采用 |
| **中** | 特定业务有效、通用性有限 | 封装为可选配置项 |
| **低** | 特定绑定、增加复杂度 | 尽量避免 |

#### 🎯 实际应用案例

**Prompt Cache功能**:

- **功能描述**: 缓存用户输入的prompt前缀，减少重复计算
- **配置方式**: 在部署配置文件里启用相关参数
- **使用透明**: 工程人员不需要额外操作
- **效果差异**: 
  - 短prompt业务: 节省几十个token的计算
  - 长prompt场景: 节省上百token，推理延迟减少几百毫秒

#### ⚠️ 工程经验总结

**框架设计原则**:

- **模块化**: 分层解耦的处理
- **透明化**: 上层调用时底层适配逻辑已统一封装
- **简化开发**: 不需要为每种技术写一套逻辑

**避免的坑**:

- **过度特定化**: 避免让"特定绑定"的优化太多
- **复杂度控制**: 防止框架变复杂，降低可维护性和易用性
- **业务绑定**: 尽量保持技术的通用性



### Q7: 端侧大模型的未来突破方向

**InfoQ**: 结合当前产业需求与技术路径，未来您认为端侧大模型最具突破性的方向或潜力业务场景会在哪里？下一阶段技术突破的核心目标可能集中在什么方向？

**杨永杰**: 前面提到的挑战有相当一部分其实都来自于端侧硬件算力不足。这也直接导致我们在部署时要做很多取舍，比如低比特量化、严格的资源控制等，都是为了适配端侧有限的计算能力。

#### 🚀 两大突破方向

**1. 硬件层面的能力突破**

**现状问题**:
- 7B大模型可以跑起来，但无法真正落地
- 运行时占用太多资源（功耗高、挤占其他业务资源）
- 导致系统卡顿等现实问题

**发展趋势**:
- 芯片厂商正在推出面向大模型的新一代端侧芯片
- 类似于当年传统模型兴起时NPU的普及
- 大模型热度持续两年多，新一代硬件即将出现

> **预期效果**: 一旦这类硬件可用，端侧模型的能力会大幅增强，更多业务有机会真正落地

**2. 模型架构的算法演进**

**当前架构限制**:

| 架构类型 | 特点 | 问题 |
|---------|------|------|
| **Transformer** | 自回归式 | Context变长时资源占用显著增加 |
| **KV Cache** | 需要保存 | 内存和算力压力不断上升 |

**新兴方向 - Linear Attention**:

- **代表架构**: Mamba、RWKV
- **核心优势**: 内存使用与输入长度无关
- **应用场景**: 更好地支持长文本推理，特别适合资源敏感的端侧场景
- **发展现状**: 效果整体还不如Transformer，但研究热度高、论文持续涌现

#### 🎯 业务场景展望

**当前交互模式**:
- 小爱目前的交互长度普遍不长
- 主要是一问一答形式

**新兴趋势**:
- **多模态任务**: 输入长度会变得很长
- **图片处理**: 一张图片转成token可能就有64个
- **视频场景**: 多个帧一起输入时，输入长度会成倍增长

**技术需求**:
- Context会迅速拉长
- 传统Transformer对资源的需求成为瓶颈
- Linear Attention可能是很好的解法

#### 🔮 总结展望

**关键突破方向**:

1. **硬件能力突破** - 面向大模型的端侧芯片
2. **算法架构演进** - Linear Attention等新技术

> **个人期待**: 这两个方向都非常关键，也都很值得期待

---

## 🎯 技术总结

### 📊 核心成果回顾

通过自研推理框架实现了**180 tokens/s**的实时推理性能，借助**LoRA插件化 + 共享基座模型**支持多业务复用，并在推理性能和资源占用上实现了极致优化。

### 🔑 关键技术突破

1. **动态输入支持**: 框架级能力支持动态输入，自动切分输入尺寸
2. **投机推理优化**: 端侧实现高达7～10倍的decoding加速
3. **共享基座架构**: 通过LoRA实现"参数共享 + 差异定制"
4. **跨平台通用性**: 模块化、后端解耦的设计适应多种芯片平台

### 🚀 未来发展方向

- **硬件突破**: 面向大模型优化的端侧芯片
- **架构演进**: Linear Attention等新模型架构
- **应用拓展**: 多模态、长文本等新业务场景

---

<div align="center">

**🔬 端侧大模型工程化的技术探索仍在继续，期待更多突破！**

</div>