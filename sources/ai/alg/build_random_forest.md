å¥½çš„ï¼Œæˆ‘æ¥è¯¦ç»†è®²è§£éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰çš„åŸç†å’Œå·¥ä½œæœºåˆ¶ã€‚

## ğŸŒ³ éšæœºæ£®æ—æ ¸å¿ƒæ€æƒ³

**éšæœºæ£®æ— = å†³ç­–æ ‘ + éšæœºæ€§ + é›†æˆå­¦ä¹ **

å®ƒæ˜¯ä¸€ç§é€šè¿‡æ„å»ºå¤šæ£µå†³ç­–æ ‘å¹¶è¿›è¡ŒæŠ•ç¥¨çš„é›†æˆå­¦ä¹ æ–¹æ³•ã€‚

## ğŸ¯ æ ¸å¿ƒåŸç†ï¼šä¸‰ä¸ªéšæœºæ€§

### 1. **Bootstrap Aggregating (Bagging)**
```python
# ä»åŸå§‹æ•°æ®ä¸­æœ‰æ”¾å›åœ°éšæœºé‡‡æ ·
def bootstrap_sample(data, n_samples):
    indices = np.random.choice(len(data), n_samples, replace=True)
    return data.iloc[indices]

# æ¯æ£µæ ‘ä½¿ç”¨ä¸åŒçš„bootstrapæ ·æœ¬
for tree in range(n_estimators):
    sample_data = bootstrap_sample(original_data, len(original_data))
    train_tree(sample_data)
```

### 2. **éšæœºç‰¹å¾é€‰æ‹©**
```python
# æ¯æ¬¡åˆ†è£‚æ—¶éšæœºé€‰æ‹©éƒ¨åˆ†ç‰¹å¾
def random_feature_subset(features, ratio=0.5):
    n_selected = int(len(features) * ratio)
    return np.random.choice(features, n_selected, replace=False)

# æ¯æ£µæ ‘åˆ†è£‚æ—¶åªè€ƒè™‘éšæœºå­é›†çš„ç‰¹å¾
selected_features = random_feature_subset(all_features, 0.6)
find_best_split(current_data, selected_features)
```

### 3. **éšæœºé˜ˆå€¼é€‰æ‹©**ï¼ˆå¯é€‰ï¼‰
æœ‰äº›å®ç°ä¸­è¿˜ä¼šå¯¹è¿ç»­ç‰¹å¾éšæœºé€‰æ‹©åˆ†è£‚é˜ˆå€¼ã€‚

## ğŸ”§ éšæœºæ£®æ—æ„å»ºæµç¨‹

### æ­¥éª¤1ï¼šå‡†å¤‡é˜¶æ®µ
```python
def build_random_forest(data, n_trees=100, feature_ratio=0.6):
    forest = []
    for i in range(n_trees):
        # 1. Bootstrapé‡‡æ ·
        bootstrap_data = bootstrap_sample(data)
        
        # 2. æ„å»ºå†³ç­–æ ‘ï¼ˆå¸¦æœ‰ç‰¹å¾éšæœºæ€§ï¼‰
        tree = build_tree(bootstrap_data, feature_ratio)
        forest.append(tree)
    
    return forest
```

### æ­¥éª¤2ï¼šå•æ£µæ ‘æ„å»º
```python
def build_tree(data, feature_ratio, max_depth=10, min_samples=5):
    # é€’å½’æ„å»ºæ ‘
    if should_stop(data, max_depth, min_samples):
        return create_leaf_node(data)
    
    # éšæœºé€‰æ‹©ç‰¹å¾å­é›†
    features = random_feature_subset(data.columns, feature_ratio)
    
    # æ‰¾åˆ°æœ€ä½³åˆ†è£‚
    best_feature, best_threshold = find_best_split(data, features)
    
    # åˆ†è£‚æ•°æ®
    left_data = data[data[best_feature] <= best_threshold]
    right_data = data[data[best_feature] > best_threshold]
    
    # é€’å½’æ„å»ºå­æ ‘
    left_tree = build_tree(left_data, feature_ratio, max_depth-1, min_samples)
    right_tree = build_tree(right_data, feature_ratio, max_depth-1, min_samples)
    
    return DecisionNode(best_feature, best_threshold, left_tree, right_tree)
```

### æ­¥éª¤3ï¼šé¢„æµ‹é˜¶æ®µ
```python
def random_forest_predict(forest, sample):
    # æ¯æ£µæ ‘è¿›è¡Œé¢„æµ‹
    predictions = []
    for tree in forest:
        pred = tree.predict(sample)
        predictions.append(pred)
    
    # å¤šæ•°æŠ•ç¥¨ï¼ˆåˆ†ç±»ï¼‰æˆ–å¹³å‡ï¼ˆå›å½’ï¼‰
    if is_classification:
        return majority_vote(predictions)
    else:
        return np.mean(predictions)
```

## ğŸ“Š ä¸ºä»€ä¹ˆéšæœºæ£®æ—æœ‰æ•ˆï¼Ÿ

### 1. **é™ä½æ–¹å·®ï¼ˆVariance Reductionï¼‰**
- å•æ£µå†³ç­–æ ‘ï¼šé«˜æ–¹å·®ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
- éšæœºæ£®æ—ï¼šå¤šæ£µæ ‘å¹³å‡ï¼Œé™ä½æ–¹å·®

### 2. **å¢åŠ å¤šæ ·æ€§ï¼ˆDiversityï¼‰**
```python
# é€šè¿‡ä¸¤ä¸ªéšæœºæ€§ç¡®ä¿æ ‘ä¹‹é—´çš„å·®å¼‚
tree_differences = []
for tree1, tree2 in combinations(forest, 2):
    similarity = calculate_tree_similarity(tree1, tree2)
    tree_differences.append(1 - similarity)

print(f"å¹³å‡æ ‘å·®å¼‚åº¦: {np.mean(tree_differences):.3f}")
```

### 3. **è¯¯å·®åˆ†è§£**
æ€»è¯¯å·® = åå·®Â² + æ–¹å·® + å™ªå£°

éšæœºæ£®æ—ä¸»è¦å‡å°‘æ–¹å·®éƒ¨åˆ†ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„åå·®ã€‚

## ğŸ¯ æ•°å­¦åŸç†ï¼šBaggingçš„å¨åŠ›

### æ–¹å·®å‡å°‘çš„æ•°å­¦è¡¨è¾¾
å¯¹äºå›å½’é—®é¢˜ï¼š
```python
# å•æ£µæ ‘çš„æ–¹å·®
single_tree_variance = ÏƒÂ²

# éšæœºæ£®æ—çš„æ–¹å·®ï¼ˆå‡è®¾æ ‘ä¹‹é—´ç›¸å…³ç³»æ•°ä¸ºÏï¼‰
forest_variance = ÏƒÂ² * (Ï + (1 - Ï)/n_trees)

# å½“n_treesâ†’âˆæ—¶ï¼Œforest_variance â†’ ÏƒÂ² * Ï
```

### æ³›åŒ–è¯¯å·®ç•Œ
éšæœºæ£®æ—çš„æ³›åŒ–è¯¯å·®ä¸Šç•Œä¸æ ‘ä¹‹é—´çš„ç›¸å…³æ€§å’Œå•æ£µæ ‘çš„è´¨é‡æœ‰å…³ã€‚

## ğŸ” éšæœºæ£®æ— vs å•å†³ç­–æ ‘

### ä¼˜åŠ¿å¯¹æ¯”ï¼š
| ç‰¹æ€§ | å•å†³ç­–æ ‘ | éšæœºæ£®æ— |
|------|----------|----------|
| **è¿‡æ‹Ÿåˆé£é™©** | é«˜ | ä½ |
| **ç¨³å®šæ€§** | ä½ï¼ˆæ•°æ®å¾®å°å˜åŒ–å¯¼è‡´å¤§ä¸åŒï¼‰ | é«˜ |
| **é¢„æµ‹ç²¾åº¦** | é€šå¸¸è¾ƒä½ | é€šå¸¸è¾ƒé«˜ |
| **è®­ç»ƒæ—¶é—´** | å¿« | æ…¢ï¼ˆä½†å¯å¹¶è¡Œï¼‰ |
| **å¯è§£é‡Šæ€§** | é«˜ | è¾ƒä½ |

## âš™ï¸ å…³é”®è¶…å‚æ•°ä½œç”¨

### 1. `n_estimators` - æ ‘çš„æ•°é‡
```python
# æ ‘è¶Šå¤šè¶Šå¥½ï¼Œä½†æœ‰æ”¶ç›Šé€’å‡
model = RandomForestModel(n_estimators=100)  # å¸¸ç”¨100-500
```
- â¬†ï¸ å¢åŠ ï¼šé™ä½æ–¹å·®ï¼Œæé«˜ç¨³å®šæ€§
- â¬‡ï¸ å‡å°‘ï¼šè®­ç»ƒå¿«ï¼Œä½†å¯èƒ½æ€§èƒ½å·®

### 2. `max_features` - ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
```python
model = RandomForestModel(num_candidate_attributes_ratio=0.6)
```
- â¬†ï¸ å¢åŠ ï¼šæ ‘ä¹‹é—´æ›´ç›¸ä¼¼ï¼Œæ–¹å·®å‡å°‘æ•ˆæœå·®
- â¬‡ï¸ å‡å°‘ï¼šæ ‘ä¹‹é—´å·®å¼‚å¤§ï¼Œä½†å•æ£µæ ‘è´¨é‡å¯èƒ½ä¸‹é™

### 3. `max_depth` - æ ‘æ·±åº¦
```python
model = RandomForestModel(max_depth=10)
```
- æ§åˆ¶å•æ£µæ ‘çš„å¤æ‚åº¦

## ğŸª å®é™…å·¥ä½œç¤ºä¾‹

### Titanicæ•°æ®é›†ä¸Šçš„éšæœºæ£®æ—
```python
import tensorflow_decision_forests as tfdf

# åˆ›å»ºéšæœºæ£®æ—æ¨¡å‹
model = tfdf.keras.RandomForestModel(
    num_trees=100,                          # 100æ£µæ ‘
    num_candidate_attributes_ratio=0.6,     # æ¯æ¬¡åˆ†è£‚è€ƒè™‘60%ç‰¹å¾
    subsample=0.8,                          # æ¯æ£µæ ‘80%æ ·æœ¬
    max_depth=8,                            # æ ‘æœ€å¤§æ·±åº¦
    min_examples=5,                         # èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    random_seed=42,                         # å¯é‡ç°æ€§
    verbose=0
)

# è®­ç»ƒæ¨¡å‹
model.fit(train_data)

# é¢„æµ‹æ—¶æ¯æ£µæ ‘æŠ•ç¥¨
predictions = model.predict(test_data)
```

## ğŸ“Š ç‰¹å¾é‡è¦æ€§è®¡ç®—

éšæœºæ£®æ—å¯ä»¥è®¡ç®—æœ‰æ„ä¹‰çš„ç‰¹å¾é‡è¦æ€§ï¼š

```python
# åŸºäºå¹³å‡çº¯åº¦å‡å°‘
feature_importance = model.make_inspector().variable_importances()["MEAN_DECREASE_IN_ACCURACY"]

# æˆ–åŸºäºæ’åˆ—é‡è¦æ€§
permutation_importance = model.make_inspector().variable_importances()["NUM_AS_ROOT"]
```

## ğŸ’¡ ä¸ºä»€ä¹ˆéšæœºæ£®æ—å¦‚æ­¤å¼ºå¤§ï¼Ÿ

1. **åŒé‡éšæœºæ€§**ï¼šæ•°æ®é‡‡æ · + ç‰¹å¾é‡‡æ ·
2. **è¯¯å·®çº æ­£**ï¼šå¤šæ£µæ ‘äº’ç›¸çº æ­£é”™è¯¯
3. **æŠ—è¿‡æ‹Ÿåˆ**ï¼šå¤©ç„¶çš„æ­£åˆ™åŒ–æ•ˆæœ
4. **å¹¶è¡ŒåŒ–**ï¼šæ¯æ£µæ ‘å¯ä»¥ç‹¬ç«‹è®­ç»ƒ

## ğŸš€ é€‚ç”¨åœºæ™¯

### ä½¿ç”¨éšæœºæ£®æ—å½“ï¼š
- âœ… éœ€è¦ç¨³å®šå¯é çš„æ€§èƒ½
- âœ… ä¸æƒ³èŠ±å¤§é‡æ—¶é—´è°ƒå‚
- âœ… æ•°æ®æœ‰å™ªå£°æˆ–ç¼ºå¤±å€¼
- âœ… éœ€è¦ç‰¹å¾é‡è¦æ€§åˆ†æ

### é¿å…ä½¿ç”¨å½“ï¼š
- âŒ éœ€è¦å®Œå…¨å¯è§£é‡Šçš„æ¨¡å‹
- âŒ æ•°æ®é‡éå¸¸å°
- âŒ é¢„æµ‹é€Ÿåº¦è¦æ±‚æé«˜

éšæœºæ£®æ—é€šè¿‡"é›†ä½“æ™ºæ…§"çš„ç†å¿µï¼Œå°†å¤šä¸ªå¼±å­¦ä¹ å™¨ï¼ˆå†³ç­–æ ‘ï¼‰ç»„åˆæˆä¸€ä¸ªå¼ºå­¦ä¹ å™¨ï¼Œåœ¨å®è·µä¸­å‡ ä¹æ€»æ˜¯æ¯”å•æ£µå†³ç­–æ ‘è¡¨ç°æ›´å¥½ã€‚



å¥½çš„ï¼Œæˆ‘æ¥è¯¦ç»†åˆ†æéšæœºæ£®æ—çš„å„ä¸ªå‚æ•°åŠå…¶ä½œç”¨ï¼š

## ğŸŒ³ éšæœºæ£®æ—å‚æ•°å…¨é¢åˆ†æ

### 1. **æ ¸å¿ƒæ•°é‡å‚æ•°**

#### `num_trees` / `n_estimators` - æ ‘çš„æ•°é‡
```python
model = tfdf.keras.RandomForestModel(num_trees=100)
```
- **ä½œç”¨**ï¼šæ£®æ—ä¸­å†³ç­–æ ‘çš„æ•°é‡
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å¤§ï¼šç¨³å®šæ€§â†‘ã€ç²¾åº¦â†‘ã€è®­ç»ƒæ—¶é—´â†‘
  - â¬‡ï¸ å€¼å°ï¼šè®­ç»ƒå¿«ã€ä½†å¯èƒ½æ–¹å·®å¤§
- **å»ºè®®**ï¼š100-500ï¼Œé€šå¸¸200æ˜¯ä¸ªå¥½èµ·ç‚¹
- **ç»éªŒ**ï¼šå¢åŠ åˆ°æ€§èƒ½ä¸å†æ˜¾è‘—æå‡ä¸ºæ­¢

#### `max_depth` - æ ‘çš„æœ€å¤§æ·±åº¦
```python
model = tfdf.keras.RandomForestModel(max_depth=10)
```
- **ä½œç”¨**ï¼šæ§åˆ¶å•æ£µæ ‘çš„å¤æ‚åº¦
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å¤§ï¼šè¿‡æ‹Ÿåˆé£é™©â†‘ã€è®­ç»ƒæ—¶é—´â†‘
  - â¬‡ï¸ å€¼å°ï¼šæ¬ æ‹Ÿåˆé£é™©â†‘ã€å¯è§£é‡Šæ€§â†‘
- **å»ºè®®**ï¼š5-15ï¼Œå¸¸ç”¨8-12

### 2. **é‡‡æ ·ç­–ç•¥å‚æ•°**

#### `subsample` - æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
```python
model = tfdf.keras.RandomForestModel(subsample=0.8)
```
- **ä½œç”¨**ï¼šæ¯æ£µæ ‘ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬æ¯”ä¾‹
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å¤§ï¼šæ ‘ä¹‹é—´ç›¸ä¼¼æ€§â†‘ã€å¤šæ ·æ€§â†“
  - â¬‡ï¸ å€¼å°ï¼šè¿‡æ‹Ÿåˆé£é™©â†“ã€ä½†å•æ£µæ ‘è´¨é‡å¯èƒ½â†“
- **å»ºè®®**ï¼š0.7-0.9ï¼Œå¸¸ç”¨0.8

#### `num_candidate_attributes_ratio` - ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
```python
model = tfdf.keras.RandomForestModel(num_candidate_attributes_ratio=0.6)
```
- **ä½œç”¨**ï¼šæ¯æ¬¡åˆ†è£‚æ—¶è€ƒè™‘çš„ç‰¹å¾æ¯”ä¾‹
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å¤§ï¼šæ ‘ä¹‹é—´æ›´ç›¸ä¼¼
  - â¬‡ï¸ å€¼å°ï¼šå¤šæ ·æ€§â†‘ã€ä½†éœ€è¦æ›´å¤šæ ‘
- **å»ºè®®**ï¼š
  - é«˜ç»´æ•°æ®ï¼š0.3-0.5
  - ä½ç»´æ•°æ®ï¼š0.5-0.8
  - é»˜è®¤ï¼šâˆš(æ€»ç‰¹å¾æ•°)/æ€»ç‰¹å¾æ•°

### 3. **åˆ†è£‚æ§åˆ¶å‚æ•°**

#### `min_examples` - èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
```python
model = tfdf.keras.RandomForestModel(min_examples=5)
```
- **ä½œç”¨**ï¼šåˆ†è£‚èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å¤§ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆã€æ³›åŒ–æ€§å¥½
  - â¬‡ï¸ å€¼å°ï¼šå¯èƒ½è¿‡æ‹Ÿåˆã€å¯¹å™ªå£°æ•æ„Ÿ
- **å»ºè®®**ï¼š
  - å¤§æ•°æ®é›†ï¼š5-10
  - å°æ•°æ®é›†ï¼š10-20

#### `split_axis` - åˆ†è£‚è½´ç­–ç•¥
```python
model = tfdf.keras.RandomForestModel(split_axis="SPARSE_OBLIQUE")
```
- **é€‰é¡¹**ï¼š
  - `"AXIS_ALIGNED"`ï¼šæ ‡å‡†è½´å¯¹é½åˆ†è£‚ï¼ˆé»˜è®¤ï¼‰
  - `"SPARSE_OBLIQUE"`ï¼šç¨€ç–æ–œåˆ†è£‚
- **å»ºè®®**ï¼š
  - ç®€å•æ•°æ®ï¼š`AXIS_ALIGNED`
  - å¤æ‚å…³ç³»ï¼š`SPARSE_OBLIQUE`

### 4. **éšæœºæ€§æ§åˆ¶å‚æ•°**

#### `random_seed` - éšæœºç§å­
```python
model = tfdf.keras.RandomForestModel(random_seed=42)
```
- **ä½œç”¨**ï¼šç¡®ä¿ç»“æœå¯é‡ç°
- **é‡è¦**ï¼šåœ¨ç§‘å­¦ç ”ç©¶ä¸­å¿…é¡»è®¾ç½®

#### `bootstrap` - æ˜¯å¦ä½¿ç”¨bootstrapé‡‡æ ·
```python
# åœ¨sklearnä¸­
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(bootstrap=True)  # é»˜è®¤True
```
- **ä½œç”¨**ï¼šæ§åˆ¶æ˜¯å¦ä½¿ç”¨æœ‰æ”¾å›é‡‡æ ·
- **å»ºè®®**ï¼šé€šå¸¸ä¿æŒTrue

## ğŸ“Š å‚æ•°è°ƒä¼˜ç­–ç•¥çŸ©é˜µ

### ä¿å®ˆé…ç½®ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
```python
model = tfdf.keras.RandomForestModel(
    num_trees=200,
    max_depth=8,
    min_examples=10,
    subsample=0.7,
    num_candidate_attributes_ratio=0.5,
    split_axis="AXIS_ALIGNED"
)
```
**é€‚ç”¨**ï¼šå°æ•°æ®é›†ã€é«˜å™ªå£°æ•°æ®

### æ¿€è¿›é…ç½®ï¼ˆè¿½æ±‚ç²¾åº¦ï¼‰
```python
model = tfdf.keras.RandomForestModel(
    num_trees=500,
    max_depth=15,
    min_examples=5,
    subsample=0.9,
    num_candidate_attributes_ratio=0.8,
    split_axis="SPARSE_OBLIQUE"
)
```
**é€‚ç”¨**ï¼šå¤§æ•°æ®é›†ã€ä½å™ªå£°ã€è¿½æ±‚æœ€ä½³æ€§èƒ½

### å¹³è¡¡é…ç½®ï¼ˆæ¨èé»˜è®¤ï¼‰
```python
model = tfdf.keras.RandomForestModel(
    num_trees=100,
    max_depth=10,
    min_examples=5,
    subsample=0.8,
    num_candidate_attributes_ratio=0.6,
    random_seed=42
)
```

## ğŸ¯ å‚æ•°ä¼˜å…ˆçº§æ’åº

1. **`num_trees`** - æœ€é‡è¦çš„å‚æ•°ï¼Œå…ˆç¡®å®šæ ‘çš„æ•°é‡
2. **`max_depth`** - æ§åˆ¶å•æ£µæ ‘å¤æ‚åº¦
3. **`num_candidate_attributes_ratio`** - æ§åˆ¶å¤šæ ·æ€§
4. **`min_examples`** - é˜²æ­¢è¿‡æ‹Ÿåˆ
5. **å…¶ä»–å‚æ•°** - æŒ‰éœ€è°ƒæ•´

## ğŸ”§ å‚æ•°è°ƒä¼˜å®æˆ˜ç¤ºä¾‹

```python
def tune_random_forest(X_train, y_train, X_val, y_val):
    best_score = 0
    best_params = {}
    
    # ç½‘æ ¼æœç´¢å…³é”®å‚æ•°
    for n_trees in [50, 100, 200]:
        for max_depth in [5, 8, 10, 15]:
            for feature_ratio in [0.3, 0.5, 0.7]:
                
                model = tfdf.keras.RandomForestModel(
                    num_trees=n_trees,
                    max_depth=max_depth,
                    num_candidate_attributes_ratio=feature_ratio,
                    verbose=0
                )
                
                model.fit(X_train, y_train)
                score = model.evaluate(X_val, y_val)
                
                if score > best_score:
                    best_score = score
                    best_params = {
                        'num_trees': n_trees,
                        'max_depth': max_depth,
                        'feature_ratio': feature_ratio
                    }
    
    return best_params, best_score
```

## ğŸ“ˆ å‚æ•°å½±å“æ€»ç»“è¡¨

| å‚æ•° | å¢å¤§æ•ˆæœ | å‡å°æ•ˆæœ | æ¨èèŒƒå›´ |
|------|----------|----------|----------|
| `num_trees` | ç¨³å®šæ€§â†‘ ç²¾åº¦â†‘ æ—¶é—´â†‘ | è®­ç»ƒå¿« æ–¹å·®å¤§ | 100-500 |
| `max_depth` | è¿‡æ‹Ÿåˆé£é™©â†‘ å¤æ‚åº¦â†‘ | æ¬ æ‹Ÿåˆé£é™©â†‘ | 5-15 |
| `subsample` | æ ‘ç›¸ä¼¼æ€§â†‘ | å¤šæ ·æ€§â†‘ è¿‡æ‹Ÿåˆâ†“ | 0.7-0.9 |
| `num_candidate_attributes_ratio` | æ ‘ç›¸ä¼¼æ€§â†‘ | å¤šæ ·æ€§â†‘ éœ€è¦æ›´å¤šæ ‘ | 0.3-0.8 |
| `min_examples` | æ³›åŒ–æ€§â†‘ è¿‡æ‹Ÿåˆâ†“ | çµæ•åº¦â†‘ è¿‡æ‹Ÿåˆâ†‘ | 5-20 |

## ğŸ’¡ å®ç”¨è°ƒå‚å»ºè®®

### 1. **å¼€å§‹æ—¶çš„é»˜è®¤é…ç½®**
```python
# è‰¯å¥½çš„èµ·ç‚¹é…ç½®
model = tfdf.keras.RandomForestModel(
    num_trees=100,
    max_depth=10,
    min_examples=5,
    num_candidate_attributes_ratio=0.6,
    random_seed=42
)
```

### 2. **æ ¹æ®æ•°æ®ç‰¹æ€§è°ƒæ•´**
- **é«˜ç»´æ•°æ®**ï¼šå‡å°`num_candidate_attributes_ratio` (0.3-0.5)
- **å°æ•°æ®é›†**ï¼šå¢å¤§`min_examples` (10-20)ï¼Œå‡å°`max_depth` (5-8)
- **å¤§æ•°æ®é›†**ï¼šå¢åŠ `num_trees` (200-500)ï¼Œå¯å¢å¤§`max_depth`

### 3. **æ€§èƒ½ä¸æ—¶é—´çš„æƒè¡¡**
```python
# å¿«é€ŸåŸå‹
quick_model = tfdf.keras.RandomForestModel(num_trees=50, max_depth=8)

# ç”Ÿäº§ç¯å¢ƒ
production_model = tfdf.keras.RandomForestModel(num_trees=300, max_depth=12)
```

## ğŸš€ æœ€ç»ˆå»ºè®®

1. **å…ˆä»é»˜è®¤å‚æ•°å¼€å§‹**ï¼Œç„¶åé€æ­¥è°ƒæ•´
2. **ä½¿ç”¨äº¤å‰éªŒè¯**è¯„ä¼°å‚æ•°æ•ˆæœ
3. **é‡ç‚¹å…³æ³¨**ï¼š`num_trees`ã€`max_depth`ã€`num_candidate_attributes_ratio`
4. **è®°ä½**ï¼šéšæœºæ£®æ—å¯¹å‚æ•°ç›¸å¯¹ä¸æ•æ„Ÿï¼Œé€šå¸¸é»˜è®¤å€¼å°±èƒ½å·¥ä½œå¾—å¾ˆå¥½

é€šè¿‡åˆç†è°ƒæ•´è¿™äº›å‚æ•°ï¼Œæ‚¨å¯ä»¥åœ¨åå·®-æ–¹å·®æƒè¡¡ä¸­æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹ï¼Œè·å¾—æ€§èƒ½ä¼˜å¼‚çš„æ¨¡å‹ã€‚