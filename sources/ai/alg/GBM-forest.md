å¥½çš„ï¼Œæˆ‘æ¥è¯¦ç»†è®²è§£æ¢¯åº¦æå‡æ ‘ï¼ˆGradient Boosting Machine, GBMï¼‰çš„å·¥ä½œåŸç†ã€‚

## ğŸ¯ GBM æ ¸å¿ƒæ€æƒ³

**GBM = å†³ç­–æ ‘ + æ¢¯åº¦ä¸‹é™ + é¡ºåºé›†æˆ**

ä¸éšæœºæ£®æ—çš„å¹¶è¡Œé›†æˆä¸åŒï¼ŒGBMé‡‡ç”¨é¡ºåºæ–¹å¼ï¼Œæ¯æ£µæ–°æ ‘éƒ½è¯•å›¾çº æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ã€‚

## ğŸ”„ GBM å·¥ä½œæµç¨‹ï¼ˆä¸‰æ­¥å¾ªç¯ï¼‰

### ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–åŸºç¡€æ¨¡å‹
```python
# åˆå§‹é¢„æµ‹ï¼ˆé€šå¸¸æ˜¯ç›®æ ‡å˜é‡çš„å‡å€¼ï¼‰
initial_prediction = np.mean(y_train)
base_predictions = np.full(len(y_train), initial_prediction)
```

### ç¬¬äºŒæ­¥ï¼šè¿­ä»£æ„å»ºæ ‘ï¼ˆæ ¸å¿ƒå¾ªç¯ï¼‰
```python
def gbm_fit(X, y, n_estimators=100, learning_rate=0.1):
    # åˆå§‹åŒ–é¢„æµ‹
    predictions = np.full(len(y), np.mean(y))
    
    trees = []
    for t in range(n_estimators):
        # 1. è®¡ç®—å½“å‰æ®‹å·®ï¼ˆè´Ÿæ¢¯åº¦ï¼‰
        residuals = y - predictions
        
        # 2. ç”¨å†³ç­–æ ‘æ‹Ÿåˆæ®‹å·®
        tree = build_decision_tree(X, residuals)
        trees.append(tree)
        
        # 3. æ›´æ–°é¢„æµ‹ï¼ˆå­¦ä¹ ç‡æ§åˆ¶æ­¥é•¿ï¼‰
        tree_predictions = tree.predict(X)
        predictions += learning_rate * tree_predictions
    
    return trees, predictions
```

### ç¬¬ä¸‰æ­¥ï¼šæœ€ç»ˆé¢„æµ‹
```python
def gbm_predict(X, trees, learning_rate, initial_prediction):
    predictions = np.full(len(X), initial_prediction)
    for tree in trees:
        predictions += learning_rate * tree.predict(X)
    return predictions
```

## ğŸ“Š æ•°å­¦åŸç†ï¼šæ¢¯åº¦ä¸‹é™è§†è§’

### æŸå¤±å‡½æ•°æœ€å°åŒ–
GBMé€šè¿‡æ¢¯åº¦ä¸‹é™æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼š

```python
# å¯¹äºå¹³æ–¹æŸå¤±å‡½æ•°
def squared_loss(y_true, y_pred):
    return 0.5 * (y_true - y_pred)**2

# è´Ÿæ¢¯åº¦ï¼ˆæ®‹å·®ï¼‰
def negative_gradient(y_true, y_pred):
    return y_true - y_pred  # å¯¹äºå¹³æ–¹æŸå¤±
```

### é€šç”¨ç®—æ³•å½¢å¼
```python
# ä¼ªä»£ç ï¼šé€šç”¨GBMç®—æ³•
for t in range(n_estimators):
    # è®¡ç®—è´Ÿæ¢¯åº¦ï¼ˆä¼ªæ®‹å·®ï¼‰
    gradients = compute_gradients(y, current_predictions, loss_function)
    
    # ç”¨æ ‘æ‹Ÿåˆè´Ÿæ¢¯åº¦
    tree = fit_tree(X, gradients)
    
    # çº¿æœç´¢æ‰¾åˆ°æœ€ä¼˜æ­¥é•¿
    step_size = line_search(y, current_predictions, tree.predict(X))
    
    # æ›´æ–°é¢„æµ‹
    current_predictions += learning_rate * step_size * tree.predict(X)
```

## ğŸ¯ GBM å¦‚ä½•çº æ­£é”™è¯¯ï¼Ÿ

### ç¤ºä¾‹ï¼šå›å½’é—®é¢˜
å‡è®¾çœŸå®å€¼ï¼š`y = [10, 20, 30]`

**è¿­ä»£1ï¼š**
- åˆå§‹é¢„æµ‹ï¼š`[20, 20, 20]`ï¼ˆå‡å€¼ï¼‰
- æ®‹å·®ï¼š`[-10, 0, 10]`
- ç¬¬ä¸€æ£µæ ‘å­¦ä¹ æ®‹å·®æ¨¡å¼

**è¿­ä»£2ï¼š**
- æ–°é¢„æµ‹ï¼š`[18, 20, 22]`
- æ–°æ®‹å·®ï¼š`[-8, 0, 8]`
- ç¬¬äºŒæ£µæ ‘è¿›ä¸€æ­¥ä¿®æ­£

**è¿­ä»£Nï¼š**
- é¢„æµ‹é€æ¸é€¼è¿‘çœŸå®å€¼

## ğŸ”§ å…³é”®ç»„ä»¶è¯¦è§£

### 1. **æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰**
```python
# ä¸åŒä»»åŠ¡ä½¿ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°
loss_functions = {
    'regression': {
        'squared': lambda y, p: 0.5 * (y - p)**2,
        'absolute': lambda y, p: abs(y - p),
        'huber': huber_loss  # å¯¹å¼‚å¸¸å€¼é²æ£’
    },
    'classification': {
        'logistic': logistic_loss,
        'exponential': exponential_loss
    }
}
```

### 2. **å­¦ä¹ ç‡ï¼ˆShrinkageï¼‰**
```python
model = tfdf.keras.GradientBoostedTreesModel(shrinkage=0.1)
```
- **ä½œç”¨**ï¼šæ§åˆ¶æ¯æ£µæ ‘çš„è´¡çŒ®ç¨‹åº¦
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å°(0.01-0.1)ï¼šéœ€è¦æ›´å¤šæ ‘ï¼Œä½†æ›´ç²¾ç¡®
  - â¬‡ï¸ å€¼å¤§(0.1-0.3)ï¼šéœ€è¦è¾ƒå°‘æ ‘ï¼Œä½†å¯èƒ½éœ‡è¡
- **å»ºè®®**ï¼šå¸¸ç”¨0.05-0.2

### 3. **æ ‘å¤æ‚åº¦æ§åˆ¶**
```python
model = tfdf.keras.GradientBoostedTreesModel(
    max_depth=6,            # æ ‘æ·±åº¦
    min_examples=5,         # èŠ‚ç‚¹æœ€å°æ ·æœ¬
    num_trees=1000          # æ ‘çš„æ•°é‡
)
```

## ğŸ“ˆ GBM ä¸éšæœºæ£®æ—çš„å¯¹æ¯”

### æ ¹æœ¬åŒºåˆ«ï¼š
| ç‰¹æ€§ | éšæœºæ£®æ— | GBM |
|------|----------|-----|
| **é›†æˆæ–¹å¼** | Baggingï¼ˆå¹¶è¡Œï¼‰ | Boostingï¼ˆé¡ºåºï¼‰ |
| **æ ‘å…³ç³»** | ç›¸äº’ç‹¬ç«‹ | ç›¸äº’ä¾èµ– |
| **å…³æ³¨ç‚¹** | é™ä½æ–¹å·® | é™ä½åå·® |
| **è®­ç»ƒé€Ÿåº¦** | å¿«ï¼ˆå¯å¹¶è¡Œï¼‰ | æ…¢ï¼ˆé¡ºåºï¼‰ |
| **è¿‡æ‹Ÿåˆ** | è¾ƒéš¾è¿‡æ‹Ÿåˆ | å®¹æ˜“è¿‡æ‹Ÿåˆ |

### è¯¯å·®å‡å°‘æ–¹å¼ï¼š
```python
# éšæœºæ£®æ—ï¼šå¹³å‡å¤šä¸ªé«˜æ–¹å·®ã€ä½åå·®çš„æ ‘
# é¢„æµ‹ = (tree1 + tree2 + ... + treeN) / N

# GBMï¼šé¡ºåºæ·»åŠ å¤šä¸ªä½æ–¹å·®ã€é«˜åå·®çš„æ ‘  
# é¢„æµ‹ = initial + Î·*tree1 + Î·*tree2 + ... + Î·*treeN
```

## ğŸ¯ GBM çš„ä¼˜åŠ¿æ‰€åœ¨

### 1. **å¼ºå¤§çš„é¢„æµ‹èƒ½åŠ›**
- é€šè¿‡é¡ºåºä¿®æ­£é”™è¯¯ï¼Œè¾¾åˆ°å¾ˆé«˜ç²¾åº¦
- åœ¨å„ç±»æœºå™¨å­¦ä¹ ç«èµ›ä¸­è¡¨ç°ä¼˜å¼‚

### 2. **çµæ´»æ€§**
- æ”¯æŒè‡ªå®šä¹‰æŸå¤±å‡½æ•°
- å¤„ç†å„ç§ç±»å‹çš„æ•°æ®

### 3. **ç‰¹å¾é‡è¦æ€§**
- æä¾›æœ‰æ„ä¹‰çš„ç‰¹å¾é‡è¦æ€§è¯„ä¼°

## âš™ï¸ GBM å…³é”®å‚æ•°åˆ†æ

### æ ¸å¿ƒå‚æ•°ç»„åˆï¼š
```python
model = tfdf.keras.GradientBoostedTreesModel(
    num_trees=1000,         # éœ€è¦è¾ƒå¤šæ ‘
    shrinkage=0.1,          # å°å­¦ä¹ ç‡
    max_depth=6,            # è¾ƒæµ…çš„æ ‘
    min_examples=10,        # é˜²æ­¢è¿‡æ‹Ÿåˆ
    early_stopping="LOSS_INCREASE",  # æ—©åœæœºåˆ¶
    random_seed=42
)
```

### å‚æ•°è°ƒä¼˜ç­–ç•¥ï¼š
```python
# å¯»æ‰¾æœ€ä½³å‚æ•°ç»„åˆ
param_grid = {
    'num_trees': [500, 1000, 2000],
    'shrinkage': [0.05, 0.1, 0.2],
    'max_depth': [4, 6, 8],
    'min_examples': [5, 10, 20]
}
```

## ğŸ” GBM çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. **è¿‡æ‹Ÿåˆé£é™©**
**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ä½¿ç”¨æ—©åœ
model = tfdf.keras.GradientBoostedTreesModel(
    early_stopping="LOSS_INCREASE",
    early_stopping_num_trees_look_ahead=10,
    validation_ratio=0.1
)

# æ­£åˆ™åŒ–
subsample=0.8,           # æ ·æœ¬é‡‡æ ·
num_candidate_attributes_ratio=0.5  # ç‰¹å¾é‡‡æ ·
```

### 2. **è®­ç»ƒæ—¶é—´é•¿çš„è§£å†³æ–¹æ¡ˆ**
```python
# ä½¿ç”¨å†å²ä¿¡æ¯åŠ é€Ÿ
use_hessian_gain=True,   # ä½¿ç”¨äºŒé˜¶å¯¼æ•°ä¿¡æ¯

# ç‰¹å¾é¢„å¤„ç†
sparse_oblique_normalization="MIN_MAX"
```

## ğŸª å®é™…å·¥ä½œç¤ºä¾‹

### Titanicæ•°æ®é›†ä¸Šçš„GBM
```python
import tensorflow_decision_forests as tfdf

# åˆ›å»ºGBMæ¨¡å‹
model = tfdf.keras.GradientBoostedTreesModel(
    num_trees=1500,
    shrinkage=0.08,
    max_depth=5,
    min_examples=8,
    subsample=0.8,
    early_stopping="LOSS_INCREASE",
    random_seed=42,
    verbose=1
)

# è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
# å¯ä»¥çœ‹åˆ°éªŒè¯æŸå¤±é€æ¸ä¸‹é™ç„¶åå¹³ç¨³
```

## ğŸ’¡ ä¸ºä»€ä¹ˆGBMå¦‚æ­¤å¼ºå¤§ï¼Ÿ

1. **é¡ºåºé”™è¯¯çº æ­£**ï¼šæ¯æ£µæ ‘ä¸“é—¨å­¦ä¹ å‰ä¸€è½®çš„æ®‹å·®
2. **æ¢¯åº¦ä¸‹é™ä¼˜åŒ–**ï¼šæ•°å­¦ä¸Šä¿è¯æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜
3. **çµæ´»æ€§**ï¼šå¯ä»¥è‡ªå®šä¹‰æŸå¤±å‡½æ•°åº”å¯¹å„ç§é—®é¢˜
4. **ç‰¹å¾ç»„åˆ**ï¼šè‡ªåŠ¨å‘ç°é‡è¦çš„ç‰¹å¾äº¤äº’

## ğŸš€ é€‚ç”¨åœºæ™¯

### ä½¿ç”¨GBMå½“ï¼š
- âœ… è¿½æ±‚æœ€é«˜é¢„æµ‹ç²¾åº¦
- âœ… æœ‰è¶³å¤Ÿè®¡ç®—èµ„æº
- âœ… éœ€è¦å¤„ç†å¤æ‚æ¨¡å¼
- âœ… å‚åŠ æœºå™¨å­¦ä¹ ç«èµ›

### é¿å…ä½¿ç”¨å½“ï¼š
- âŒ è®­ç»ƒæ—¶é—´å—é™
- âŒ éœ€è¦æå¿«çš„é¢„æµ‹é€Ÿåº¦
- âŒ æ•°æ®é‡éå¸¸å°
- âŒ éœ€è¦å®Œå…¨æ¨¡å‹å¯è§£é‡Šæ€§

GBMé€šè¿‡å°†å¤šä¸ªå¼±å­¦ä¹ å™¨ï¼ˆæµ…å±‚å†³ç­–æ ‘ï¼‰é¡ºåºç»„åˆï¼Œæ¯ä¸ªæ–°å­¦ä¹ å™¨éƒ½ä¸“æ³¨äºçº æ­£å‰ä¸€ä¸ªçš„é”™è¯¯ï¼Œä»è€Œæ„å»ºå‡ºä¸€ä¸ªæå…¶å¼ºå¤§çš„é¢„æµ‹æ¨¡å‹ã€‚


å¥½çš„ï¼Œæˆ‘æ¥è¯¦ç»†è§£é‡ŠGBMï¼ˆGradient Boosted Treesï¼‰çš„å„ä¸ªå‚æ•°åŠå…¶ä½œç”¨ï¼š

## ğŸ¯ GBM å‚æ•°å…¨é¢è§£æ

### 1. **æ ¸å¿ƒè¿­ä»£å‚æ•°**

#### `num_trees` - æ ‘çš„æ•°é‡
```python
model = tfdf.keras.GradientBoostedTreesModel(num_trees=1000)
```
- **ä½œç”¨**ï¼šBoostingè¿­ä»£æ¬¡æ•°ï¼Œå³æ ‘çš„æ•°é‡
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å¤§ï¼šç²¾åº¦â†‘ã€è¿‡æ‹Ÿåˆé£é™©â†‘ã€è®­ç»ƒæ—¶é—´â†‘
  - â¬‡ï¸ å€¼å°ï¼šæ¬ æ‹Ÿåˆé£é™©â†‘
- **å»ºè®®**ï¼š500-2000ï¼Œéœ€è¦é…åˆæ—©åœä½¿ç”¨
- **æ³¨æ„**ï¼šGBMéœ€è¦æ¯”éšæœºæ£®æ—æ›´å¤šçš„æ ‘

#### `shrinkage` / `learning_rate` - å­¦ä¹ ç‡
```python
model = tfdf.keras.GradientBoostedTreesModel(shrinkage=0.1)
```
- **ä½œç”¨**ï¼šæ§åˆ¶æ¯æ£µæ ‘çš„è´¡çŒ®ç¨‹åº¦
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å°(0.01-0.1)ï¼šå­¦ä¹ æ…¢ã€éœ€è¦æ›´å¤šæ ‘ã€æ›´ç²¾ç¡®
  - â¬‡ï¸ å€¼å¤§(0.1-0.3)ï¼šå­¦ä¹ å¿«ã€éœ€è¦è¾ƒå°‘æ ‘ã€å¯èƒ½éœ‡è¡
- **å»ºè®®**ï¼š0.05-0.2
- **ç»éªŒ**ï¼šå°å­¦ä¹ ç‡+å¤šæ ‘é€šå¸¸æ•ˆæœæ›´å¥½

### 2. **æ ‘ç»“æ„å‚æ•°**

#### `max_depth` - æ ‘çš„æœ€å¤§æ·±åº¦
```python
model = tfdf.keras.GradientBoostedTreesModel(max_depth=6)
```
- **ä½œç”¨**ï¼šæ§åˆ¶å•æ£µæ ‘çš„å¤æ‚åº¦
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å¤§ï¼šæ•æ‰å¤æ‚æ¨¡å¼èƒ½åŠ›â†‘ã€è¿‡æ‹Ÿåˆé£é™©â†‘
  - â¬‡ï¸ å€¼å°ï¼šæ¨¡å‹æ›´ç®€å•ã€åå·®â†‘
- **å»ºè®®**ï¼š3-8ï¼ˆGBMé€šå¸¸ä½¿ç”¨è¾ƒæµ…çš„æ ‘ï¼‰

#### `min_examples` - èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
```python
model = tfdf.keras.GradientBoostedTreesModel(min_examples=10)
```
- **ä½œç”¨**ï¼šåˆ†è£‚èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å¤§ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆã€æ³›åŒ–æ€§å¥½
  - â¬‡ï¸ å€¼å°ï¼šå¯èƒ½è¿‡æ‹Ÿåˆã€å¯¹å™ªå£°æ•æ„Ÿ
- **å»ºè®®**ï¼š5-20

### 3. **æ­£åˆ™åŒ–å‚æ•°**

#### `subsample` - æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
```python
model = tfdf.keras.GradientBoostedTreesModel(subsample=0.8)
```
- **ä½œç”¨**ï¼šæ¯æ£µæ ‘ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬æ¯”ä¾‹
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å°ï¼šéšæœºæ€§â†‘ã€è¿‡æ‹Ÿåˆé£é™©â†“
  - â¬‡ï¸ å€¼å¤§ï¼šæ ‘ä¹‹é—´æ›´ç›¸ä¼¼
- **å»ºè®®**ï¼š0.7-0.9

#### `num_candidate_attributes_ratio` - ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
```python
model = tfdf.keras.GradientBoostedTreesModel(num_candidate_attributes_ratio=0.5)
```
- **ä½œç”¨**ï¼šæ¯æ¬¡åˆ†è£‚æ—¶è€ƒè™‘çš„ç‰¹å¾æ¯”ä¾‹
- **å½±å“**ï¼š
  - â¬†ï¸ å€¼å°ï¼šå¤šæ ·æ€§â†‘ã€è¿‡æ‹Ÿåˆé£é™©â†“
  - â¬‡ï¸ å€¼å¤§ï¼šæ ‘ä¹‹é—´æ›´ç›¸ä¼¼
- **å»ºè®®**ï¼š0.3-0.7

### 4. **æ—©åœå‚æ•°**

#### `early_stopping` - æ—©åœç­–ç•¥
```python
model = tfdf.keras.GradientBoostedTreesModel(
    early_stopping="LOSS_INCREASE",
    early_stopping_num_trees_look_ahead=10,
    validation_ratio=0.1
)
```
- **é€‰é¡¹**ï¼š
  - `"LOSS_INCREASE"`ï¼šæŸå¤±å¢åŠ æ—¶åœæ­¢
  - `"NONE"`ï¼šä¸ä½¿ç”¨æ—©åœ
- **ä½œç”¨**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè‡ªåŠ¨ç¡®å®šæœ€ä½³æ ‘æ•°é‡
- **å»ºè®®**ï¼šå¿…é¡»ä½¿ç”¨ï¼

#### `validation_ratio` - éªŒè¯é›†æ¯”ä¾‹
```python
model = tfdf.keras.GradientBoostedTreesModel(validation_ratio=0.1)
```
- **ä½œç”¨**ï¼šç”¨äºæ—©åœçš„å†…éƒ¨éªŒè¯é›†æ¯”ä¾‹
- **å»ºè®®**ï¼š0.1-0.2

### 5. **é«˜çº§ä¼˜åŒ–å‚æ•°**

#### `split_axis` - åˆ†è£‚è½´ç­–ç•¥
```python
model = tfdf.keras.GradientBoostedTreesModel(split_axis="SPARSE_OBLIQUE")
```
- **ä½œç”¨**ï¼šæ§åˆ¶å¦‚ä½•å¯»æ‰¾æœ€ä½³åˆ†è£‚ç‚¹
- **é€‰é¡¹**ï¼š
  - `"AXIS_ALIGNED"`ï¼šæ ‡å‡†åˆ†è£‚ï¼ˆé»˜è®¤ï¼‰
  - `"SPARSE_OBLIQUE"`ï¼šæ–œåˆ†è£‚ï¼Œæ•æ‰å¤æ‚å…³ç³»

#### `use_hessian_gain` - ä½¿ç”¨äºŒé˜¶å¯¼æ•°
```python
model = tfdf.keras.GradientBoostedTreesModel(use_hessian_gain=True)
```
- **ä½œç”¨**ï¼šæ˜¯å¦ä½¿ç”¨äºŒé˜¶å¯¼æ•°ä¿¡æ¯ä¼˜åŒ–åˆ†è£‚
- **å½±å“**ï¼šé€šå¸¸èƒ½è·å¾—æ›´å¥½çš„åˆ†è£‚ç‚¹
- **å»ºè®®**ï¼šTrueï¼ˆå¦‚æœè®¡ç®—èµ„æºå…è®¸ï¼‰

## ğŸ“Š å‚æ•°è°ƒä¼˜ç­–ç•¥çŸ©é˜µ

### ä¿å®ˆé…ç½®ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
```python
model = tfdf.keras.GradientBoostedTreesModel(
    num_trees=2000,         # å¤šæ ‘
    shrinkage=0.05,         # å°å­¦ä¹ ç‡
    max_depth=4,            # æµ…æ ‘
    min_examples=15,        # éœ€è¦æ›´å¤šæ ·æœ¬
    subsample=0.7,          # æ ·æœ¬é‡‡æ ·
    num_candidate_attributes_ratio=0.4,  # ç‰¹å¾é‡‡æ ·
    early_stopping="LOSS_INCREASE",
    validation_ratio=0.1
)
```

### æ¿€è¿›é…ç½®ï¼ˆè¿½æ±‚ç²¾åº¦ï¼‰
```python
model = tfdf.keras.GradientBoostedTreesModel(
    num_trees=1000,
    shrinkage=0.2,          # å¤§å­¦ä¹ ç‡
    max_depth=8,            # æ·±æ ‘
    min_examples=5,         # å®¹æ˜“åˆ†è£‚
    subsample=0.9,          # å¤šç”¨æ•°æ®
    num_candidate_attributes_ratio=0.8,  # å¤šç”¨ç‰¹å¾
    early_stopping="LOSS_INCREASE"
)
```

### å¹³è¡¡é…ç½®ï¼ˆæ¨èé»˜è®¤ï¼‰
```python
model = tfdf.keras.GradientBoostedTreesModel(
    num_trees=1500,
    shrinkage=0.1,
    max_depth=6,
    min_examples=8,
    subsample=0.8,
    num_candidate_attributes_ratio=0.6,
    early_stopping="LOSS_INCREASE",
    validation_ratio=0.1,
    random_seed=42
)
```

## ğŸ¯ å‚æ•°ä¼˜å…ˆçº§æ’åº

1. **`shrinkage` + `num_trees`** - æœ€é‡è¦çš„ç»„åˆ
2. **`max_depth`** - æ§åˆ¶æ¨¡å‹å¤æ‚åº¦
3. **`early_stopping`** - é˜²æ­¢è¿‡æ‹Ÿåˆçš„å…³é”®
4. **`subsample`** - æ­£åˆ™åŒ–çš„é‡è¦å‚æ•°
5. **å…¶ä»–å‚æ•°** - æŒ‰éœ€å¾®è°ƒ

## ğŸ”§ å…³é”®å‚æ•°äº¤äº’æ•ˆåº”

### å­¦ä¹ ç‡ä¸æ ‘æ•°é‡çš„æƒè¡¡
```python
# æ–¹æ¡ˆAï¼šå¤§å­¦ä¹ ç‡ï¼Œå°‘æ ‘
model_A = GradientBoostedTreesModel(shrinkage=0.3, num_trees=300)

# æ–¹æ¡ˆBï¼šå°å­¦ä¹ ç‡ï¼Œå¤šæ ‘  
model_B = GradientBoostedTreesModel(shrinkage=0.05, num_trees=2000)

# é€šå¸¸æ–¹æ¡ˆBæ•ˆæœæ›´å¥½ä½†è®­ç»ƒæ›´æ…¢
```

### æ ‘æ·±åº¦ä¸å­¦ä¹ ç‡çš„é…åˆ
```python
# æ·±æ ‘éœ€è¦æ›´å°çš„å­¦ä¹ ç‡
deep_tree_config = GradientBoostedTreesModel(
    max_depth=8,
    shrinkage=0.05,  # å°å­¦ä¹ ç‡é…åˆæ·±æ ‘
    num_trees=2000
)

# æµ…æ ‘å¯ä»¥ç”¨å¤§ä¸€ç‚¹çš„å­¦ä¹ ç‡
shallow_tree_config = GradientBoostedTreesModel(
    max_depth=4, 
    shrinkage=0.2,   # å¤§å­¦ä¹ ç‡é…åˆæµ…æ ‘
    num_trees=500
)
```

## ğŸ“ˆ å‚æ•°å½±å“æ€»ç»“è¡¨

| å‚æ•° | å¢å¤§æ•ˆæœ | å‡å°æ•ˆæœ | æ¨èèŒƒå›´ |
|------|----------|----------|----------|
| `num_trees` | ç²¾åº¦â†‘ è¿‡æ‹Ÿåˆâ†‘ æ—¶é—´â†‘ | æ¬ æ‹Ÿåˆé£é™©â†‘ | 500-2000 |
| `shrinkage` | å­¦ä¹ å¿« æ ‘å°‘ éœ‡è¡â†‘ | å­¦ä¹ æ…¢ æ ‘å¤š ç²¾ç¡®â†‘ | 0.05-0.2 |
| `max_depth` | å¤æ‚åº¦â†‘ è¿‡æ‹Ÿåˆâ†‘ | åå·®â†‘ ç®€å•â†‘ | 3-8 |
| `min_examples` | æ³›åŒ–æ€§â†‘ è¿‡æ‹Ÿåˆâ†“ | è¿‡æ‹Ÿåˆé£é™©â†‘ | 5-20 |
| `subsample` | è¿‡æ‹Ÿåˆé£é™©â†‘ | éšæœºæ€§â†‘ è¿‡æ‹Ÿåˆâ†“ | 0.7-0.9 |

## ğŸ’¡ å®ç”¨è°ƒå‚æŒ‡å—

### 1. **å¼€å§‹æ—¶çš„é»˜è®¤é…ç½®**
```python
# è‰¯å¥½çš„èµ·ç‚¹
model = tfdf.keras.GradientBoostedTreesModel(
    num_trees=1000,
    shrinkage=0.1, 
    max_depth=6,
    min_examples=10,
    early_stopping="LOSS_INCREASE",
    random_seed=42
)
```

### 2. **æ ¹æ®æ•°æ®ç‰¹æ€§è°ƒæ•´**
- **å¤§æ•°æ®é›†**ï¼šå¯å¢åŠ `max_depth`ï¼Œå‡å°‘`min_examples`
- **å°æ•°æ®é›†**ï¼šå‡å°‘`max_depth`ï¼Œå¢åŠ `min_examples`
- **é«˜å™ªå£°æ•°æ®**ï¼šå‡å°`shrinkage`ï¼Œå¢åŠ `subsample`é‡‡æ ·

### 3. **è°ƒå‚é¡ºåºå»ºè®®**
1. å›ºå®š`shrinkage=0.1`ï¼Œè°ƒ`num_trees`ï¼ˆç”¨æ—©åœï¼‰
2. è°ƒ`max_depth`ï¼ˆ4,6,8ï¼‰
3. è°ƒ`shrinkage`ï¼ˆ0.05,0.1,0.2ï¼‰
4. è°ƒæ­£åˆ™åŒ–å‚æ•°ï¼ˆ`subsample`, `min_examples`ï¼‰

## ğŸš€ æœ€ç»ˆå»ºè®®

1. **å¿…é¡»ä½¿ç”¨æ—©åœ**ï¼š`early_stopping="LOSS_INCREASE"`
2. **å°å­¦ä¹ ç‡+å¤šæ ‘**ï¼šé€šå¸¸æ¯”å¤§å­¦ä¹ ç‡+å°‘æ ‘æ•ˆæœæ›´å¥½
3. **äº¤å‰éªŒè¯**ï¼šGBMå¯¹å‚æ•°æ•æ„Ÿï¼Œä¸€å®šè¦ç”¨CVéªŒè¯
4. **è€å¿ƒè°ƒå‚**ï¼šGBMè°ƒå‚éœ€è¦æ—¶é—´ï¼Œä½†å›æŠ¥å¾ˆé«˜

é€šè¿‡åˆç†è°ƒæ•´è¿™äº›å‚æ•°ï¼Œæ‚¨å¯ä»¥åœ¨åå·®-æ–¹å·®æƒè¡¡ä¸­æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹ï¼Œå……åˆ†å‘æŒ¥GBMçš„å¼ºå¤§é¢„æµ‹èƒ½åŠ›ã€‚