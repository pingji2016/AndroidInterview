# 学习率详解：深度学习优化的关键参数

太好了！这是深度学习中最重要、也是最微妙的超参数之一。理解学习率是模型调优的关键。

学习率（Learning Rate）是什么？

一句话概括：学习率决定了优化器在每次迭代中更新模型参数的步长有多大。

- 回顾优化器的更新公式：$$w_{new} = w_{old} - \eta \cdot \nabla_w L$$
  - $$w$$：模型参数（权重）
  - $$\nabla_w L$$：损失函数关于参数 $$w$$ 的**梯度**（指明了下降方向）
  - $$\eta$$（读作“eta”）：**学习率**（决定了沿着这个方向**走多远**）
    
一个经典的比喻：蒙眼下山
你的目标是走到山谷最低点（损失最小点）。
- 梯度告诉你哪个方向是**下坡**。
- 学习率就是你**每一步迈多大**。
  - 步子太大（**学习率太高**）：你可能会直接跨过山谷最低点，甚至在山谷两边来回横跳，无法收敛。
  - 步子太小（**学习率太低**）：你下山会非常慢，需要走很多很多步，训练时间漫长，而且可能卡在一个小坑里（局部最优点）就以为到谷底了。
    
所以，学习率直接控制了模型学习的**速度**和**稳定性**。


---

学习率一般是多少？

这是最核心的问题，也是一个陷阱题。没有一个放之四海而皆准的“最佳”学习率值。

学习率的设定高度依赖于：
- 模型结构（CNN, Transformer等）
- 优化器类型（SGD, Adam等）
- 数据集（图片、文本等）
- 批次大小（Batch Size）
  
但是，有一些广泛使用的经验法则和起始点：

1. 对于不同的优化器，默认学习率不同

- Adam / AdamW 优化器：
  - 默认/常用值：3e-4， 1e-3， 5e-4
  - 为什么？ 因为Adam自适应地为每个参数计算学习率，所以它对初始学习率不那么敏感。3e-4 (0.0003) 被很多学者（比如OpenAI）认为是是一个“在大多数情况下都还不错”的默认值。
  - 常见范围：一般介于 1e-5 (0.00001) 到 1e-2 (0.01) 之间。
    
- SGD / SGD with Momentum 优化器：
  - 常用值：0.01， 0.1， 0.001
  - 为什么？ SGD对学习率更敏感，通常需要比Adam更大的学习率。0.01 或 0.1 是一个常见的起点。
  - 常见范围：一般介于 1e-3 (0.001) 到 1 之间。
    
实践建议：当你使用Adam时，可以从 3e-4 开始尝试。

2. 一个重要的策略：学习率衰减（Learning Rate Decay）

我们并不总是使用一个固定不变的学习率。一个常见的策略是**在训练过程中逐渐减小学习率**。

- 为什么？
  - 训练初期：损失较大，距离最优解较远，我们需要较大的步长快速逼近。
  - 训练后期：已经接近最优解，太大的步长会在最优点附近来回震荡，无法收敛。此时需要很小的步长来进行“精调”。
    
- 常见衰减方式：
  - 步长衰减（Step Decay）：每训练N个回合（Epoch），将学习率乘以一个衰减系数（如0.1）。
  - 余弦退火（Cosine Annealing）：学习率随着训练过程按余弦函数的形状从初始值衰减到0。这是目前非常流行且有效的方法。
  - ** warmup（预热）**：在训练最开始的一小部分时间里，学习率从一个很小的值（如0）线性增加到预设的初始值。这有助于训练初期模型的稳定性，在训练Transformer等大模型时几乎是标配。
    

---

如何选择学习率？—— 学习率寻找（Learning Rate Finding）

盲目尝试不同学习率效率很低。有一个非常著名的技巧叫做**学习率范围测试（LR Range Test）**：

1. 开始训练，但**从一个非常小的学习率（如 1e-8）开始**。
2. 每个批次（Batch）后，**指数级地增加学习率**（例如，每次乘以1.1）。
3. 记录每个学习率对应的**损失值**。
4. 绘制**学习率 vs. 损失**的曲线图。
5. 在图中找到**损失下降最快**的那个区域所对应的学习率。通常你会选择比“损失最低点”稍微小一点的值作为你的初始学习率。
  
![Learning Rate Finder Plot](https://miro.medium.com/v2/resize:fit:1400/1*_6MfwkkTdWQZx0O

交叉验证
随机森林和SVM
